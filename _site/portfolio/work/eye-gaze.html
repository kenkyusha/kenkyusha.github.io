<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Human Eye-Gaze Tracking | Arne  Niitsoo</title>
    <meta name="author" content="Arne  Niitsoo">
    <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">
    <meta name="keywords" content="data science, machine learning, deep learning, recommendation systems">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/portfolio/work/eye-gaze">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Arne </span>Niitsoo</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Human Eye-Gaze Tracking</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <!-- <h3>70 % complete</h3> -->
<!-- ![Gaze-tracking](/img/projects/eye-gaze.jpg) -->
<p>
  In the beginning of 2020 I was working in a early-stage startup in Paris focusing on computer vision solutions. One of the projects, which I was more envolved in was human eye-gaze tracking using the laptops camera. 
</p>

<!-- ![Humaneye](/img/projects/gaze/1.jpg) -->
<h2 id="human-eye">Human Eye</h2>
<div style="text-align:center"><img src="/assets/img/projects/gaze/1.jpg"></div>
<p><br></p>

<p>The human eye is very much like a camera, it has:</p>
<ul>
  <li>
<strong>a lens</strong> - directing the light rays onto the screen (the retina)</li>
  <li>
<strong>an iris</strong> - controlling the opening of the pupil</li>
  <li>
<strong>a pupil</strong> - the hole in the middle of the iris, through which the light passes</li>
  <li>
<strong>a cornea</strong> - outer transparent covering on the pupil</li>
  <li>
<strong>a retina</strong> - onto which the light coming after it has passed through the pupil and refracted through the lens</li>
</ul>

<div style="text-align:center"><img src="/assets/img/projects/gaze/2.jpg"></div>
<p><br></p>

<p>The challenge in human-eye gaze detection and projection comes from something very unexpected. Namely, the <strong>optical axis</strong>, which we can extract from images using different eye tracking techniques is not the same as the <strong>visual gaze(axis)</strong>, where the sharpest image is formed. The line connecting from the <strong>fovea</strong> (where focused point is the sharpest) to the center of the lense is called the visual axis. The difference between visual and optical axis is called the <strong>kappa angle</strong> and this is different for each of us. This is why most eye-gaze tracking solutions need some form of person specific calibration, in order to estimate this angle.</p>

<p>As I started working on this problem I quickly noticed that there are very many different approaches to solve this problem:</p>

<ul>
  <li>3D model based - using the geometric model of human face</li>
  <li>Regression based - extracting features from 2D image and train a classifier</li>
  <li>Cross-ratio based - needing 4 different light sources, hardware dependant</li>
  <li>Apperance based - using the extracted face or the images of eyes to train a classifier</li>
</ul>

<h2 id="apperance-based-gaze">Apperance-based gaze</h2>
<p>In my work, I focused on the apperance based methods and luckily for me a lot of the work was already done and released publicly by
<a href="https://github.com/hysts/pytorch_mpiigaze_demo" target="_blank" rel="external nofollow noopener">Zhang et al.</a>
They released two different models one using the left and the right eye and other one using the full face for prediction of the gaze angles.</p>

<div style="text-align:center"><img src="/assets/img/projects/gaze/3.jpg" width="100%"></div>
<p><br></p>

<p>The process is roughly divided into 5 steps, where first the image is calibrated and undistorted, followed by face detectiong using the dlib face-detector. The landmarks detected by the dlib library are then used to estimate the head pose and fit the 3D face landmark model. This is useful for estimating the pupil center and hence extracting the left and the right eye from the image. Then the images of eyes are extracted, normalized and combined into 2 channel image. Then they are fed into a model, which returns gaze vectors (pitch and yaw) for each eye.</p>

<p>In order to make this all work a good calibration is needed. This calibration is hardware specific in order to obtain the camera intrinsic parameters and is usually done using <a href="https://docs.opencv.org/2.4/doc/tutorials/calib3d/camera_calibration/camera_calibration.html" target="_blank" rel="external nofollow noopener">checkerboard calibration.</a> 
The rest is using the estimated center of the eye and the predicted gaze vectors in order to calculate the approximate position on the screen. The resulting accuracy is actually pretty good considering that there is no person-specific calibration.</p>

<h2 id="performance">Performance</h2>
<p>In order to make the system more stable, I have done averaging over the facial landmark in order to keep the extracted images bit more stable, furthermore I am doing averaging over the predicted gaze vectors</p>

<div style="text-align:center"><img src="/assets/img/projects/gaze/4.pdf" width="90%"></div>
<p><br></p>

<p>Here we can see the result of focusing 5 different points on the screen w.r.t. some metrics such as <a href="https://en.wikipedia.org/wiki/Mean_absolute_error" target="_blank" rel="external nofollow noopener">mean absolute error (MAE)</a>, CEP and CE95. Note that blinking is not filtered.</p>

<div style="text-align:center"><img src="/assets/img/projects/gaze/5.pdf" width="90%"></div>
<p><br></p>

<p>In the figure above, we can see how the MAE changes according to the distance (i.e. we fix the distance). The graphs show the calculated MAE  w.r.t. each eye and if we would average them together over different distances. This plots gives us hints on what could be done in order to improve the model.</p>

<p>Check out more at my <a href="https://github.com/kenkyusha/eyeGazeToScreen" target="_blank" rel="external nofollow noopener">github page.</a></p>

          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Arne  Niitsoo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-177262811-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-177262811-1');
  </script><!-- Panelbear Analytics - We respect your privacy -->
  <script async src="https://cdn.panelbear.com/analytics.js?site=5AKp1yyI9O1"></script>
  <script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: '5AKp1yyI9O1' });
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
