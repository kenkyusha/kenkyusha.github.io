<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    
    <!-- Website verification -->
    <meta name="google-site-verification" content="">

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Multi-Sensor Data Acquisition Platform | Arne  Niitsoo</title>
    <meta name="author" content="Arne  Niitsoo">
    <meta name="description" content="Arne Niitsoo's personal web site at http://kenkyusha.github.io/.
">
    <meta name="keywords" content="data science, machine learning, deep learning, recommendation systems">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="http://localhost:4000/portfolio/work/dap">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Arne </span>Niitsoo</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Multi-Sensor Data Acquisition Platform</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <p>
  In early 2018, as I was working at the Fraunhofer IIS, one of the exciting projects I was working on involved several sensors such as camera, RADAR, LIDAR and the problem to be solved was pedestrian detection. The whole idea resolved around the theme of autonomous systems (driving) and part of the work was to find out how can we use consumer good radar and lidar sensors instead of expensive systems such as Velodyne Lidar costing more than 10K back at that time.
</p>

<h2 id="sensors-lidar-radar-camera">Sensors: LIDAR, RADAR, CAMERA</h2>

<div style="text-align:center"><img src="/assets/img/projects/dap/3.jpg"></div>
<p><br>
In my disposal was the RPLidar A2 (left), TimeDomain P440 RADAR (middle) and Logitech C270 cameras (right) depicted above. 
One of the problems with cheap sensors is the lack of data. What this means that for using machine learning methods, everything had to be measured and labeled from scratch.<br>
Neither the RADAR or the lidar had any pre-existing datasets such as the more expensive systems such as the Velodyne Lidar, which have datasets of point clouds from the real environment.</p>

<h2 id="platform">Platform</h2>
<div style="text-align:center"><img src="/assets//img/projects/dap/4.jpg" width="100%"></div>
<p><br>
Another problem at hand was that the system had to run on a embedded computing device from NVIDIA such as the <a href="https://developer.nvidia.com/embedded/jetson-tx2" target="_blank" rel="external nofollow noopener">Jetson TX2</a>. Thus, the computational limitations, were very strict considering how much processing could be done real-time. The idea was to build a camera field of view of 360 degrees, such that we would attach 8 Logitech Cameras on a platter covering the whole surrounding. On top of the camera array, on another level was sitting the Jetson TX2 board with the TimeDomain radar and on the third “floor” the lidar in order have the least occlusion with other obstacles. Now that we had the platform set for the first part i.e. the data recording and gathering, we could start making measurements.</p>

<h2 id="data-acquisition">Data Acquisition</h2>
<div style="text-align:center"><img src="/assets/img/projects/dap/5.jpg" width="80%"></div>
<p>Working with data, labeling and measurements is one of the most time-consuming effort. It is very inefficient to label the data manually, hence more efficent methods are desirable. 
One of the benefits of having a camera sensor is that, there is already bunch of really good neural network based object detection algorithms. And this is a huge benefit, the camera detection could easily be mapped into the lidar domain, from where the distance to the object could be extracted. This distance could be then used to find the object reflection peak in the RADAR domain.. et voila! We can do semi-automatic labeleing! The challenge now was to make enough recordings and preprocess the measured data on a more powerful computer with good object detector algorithm. In this particular project I used the <a href="https://github.com/facebookresearch/Detectron" target="_blank" rel="external nofollow noopener">Facebook Detectron</a>.</p>

<h2 id="demo-platform">Demo Platform</h2>
<p>Since there are only limited computational resources available. Instead of using the all of the cameras, only one camera together with lidar and radar are used for deployment. This is sufficient in terms of what we want to achieve, namely to detect what is in front of the autonomous system, using a neural network based object detector (<a href="https://github.com/chuanqi305/ssd" target="_blank" rel="external nofollow noopener">Single Shot Detector</a> running at 15FPS on Jetson TX2), classify it and track it. And as soon as, there is a detection of a person, we track them until they have left the operational zone of the platform using the data points in lidar and radar domains.</p>

<div style="text-align:center"><img src="/assets/img/projects/dap/towards_dap_big.gif" width="70%"></div>
<p><br>
Above we can see, the detection in the camera domain, which initiates the  tracking the points in the lidar domain. In this example, we can see how the system works walking towards and backwards from the sensor. In the lidar domain even the static points are flickering (makes the problem bit harder), but as we can see the points associated with the person are clearly distinguishable. Observing the radar domain, we can see the red dots moving closer and further away on the waterfall plot.</p>

<div style="text-align:center"><img src="/assets/img/projects/dap/out_fov_big.gif" width="70%"></div>
<p><br>
In the gif above, we can see the person being tracked in the camera field of view and at the same time in the lidar and radar domain. Notice, how the tracking persist in the lidar and radar domain after the person has already left the camera field of view.</p>

<h2 id="conclusion">Conclusion</h2>
<p>This was one of the most challenging and exciting projects I’ve worked on. The beauty of this project was that I could combine my previously acquired knowledge in machine learning, take some state of the art classifiers such as the FB Detectron for offline Semi-autonomous data labeleing and Single Shot Detector (SSD) for the live-demo version. The hardest part was the combination and synchronization of the different data streams from sensors. For the live-demo each of the sensor had to be tuned down to work at the rate of the SSD (image detector) speed as this was the most compute heavy task.</p>


          </article>

        </div>

    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Arne  Niitsoo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script defer src="/assets/js/common.js"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-177262811-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'UA-177262811-1');
  </script><!-- Panelbear Analytics - We respect your privacy -->
  <script async src="https://cdn.panelbear.com/analytics.js?site=5AKp1yyI9O1"></script>
  <script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: '5AKp1yyI9O1' });
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
